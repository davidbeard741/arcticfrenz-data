{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1JsZICY6bIZ7-T133e_mw12Zv_ehrBKJy",
      "authorship_tag": "ABX9TyNvTId37pzoBMo1/hSksYnN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraper"
      ],
      "metadata": {
        "id": "nEcC25TPcacs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Started"
      ],
      "metadata": {
        "id": "2eNjymAuMNaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xNImgY6AIfRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraper With Python Tutorial"
      ],
      "metadata": {
        "id": "oeU0bNjSdB5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Scraper with Python Tutorial\n",
        "\n",
        "```\n",
        "# Import the Requests library for making HTTP requests\n",
        "import requests\n",
        "\n",
        "# Install Beautiful Soup, a Python library for parsing structured data\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# set the URL to parse\n",
        "URL = \"https://realpython.github.io/fake-jobs/\"\n",
        "\n",
        "# Printing the `.text` attribute of a page provides a holistic, quick look at the HTML.\n",
        "page = requests.get(URL)\n",
        "print(page.text)\n",
        "\n",
        "# To save the page, use the binary `.content` attribute.\n",
        "with open(\"/content/drive/MyDrive/AF/arcticfrenz.html\", \"wb\") as f:\n",
        "    f.write(page.content)\n",
        "\n",
        "# Create a Beautiful Soup object that takes `page.content` as its input.\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "# Find specific HTML elements by its ID\n",
        "results = soup.find(id=\"ResultsContainer\")\n",
        "\n",
        "# For easier viewing, any Beautiful Soup object can use prettify\n",
        "print(results.prettify())\n",
        "\n",
        "\n",
        "# Find elements by HTML class name\n",
        "job_elements = results.find_all(\"div\", class_=\"card-content\")\n",
        "\n",
        "# Pick out child HTML elements with `.find()` attribute.\n",
        "# Each job_element is another BeautifulSoup() object.\n",
        "# `.strip()` can be used to remove the superfluous whitespace\n",
        "for job_element in job_elements:\n",
        "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
        "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
        "    location_element = job_element.find(\"p\", class_=\"location\")\n",
        "    print(title_element.text.strip())\n",
        "    print(company_element.text.strip())\n",
        "    print(location_element.text.strip())\n",
        "    print()\n",
        "\n",
        "# To find elements by class name and text context you can use the string argument\n",
        "# And you can sometimes pass functions as arguments to Beautiful Soup methods\n",
        "python_jobs = results.find_all(\n",
        "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
        ")\n",
        "\n",
        "# A method for checking is to use `len()`\n",
        "print(len(python_jobs))\n",
        "\n",
        "# You can fetch great-grandparent elements\n",
        "python_job_elements = [\n",
        "    h2_element.parent.parent.parent for h2_element in python_jobs\n",
        "]\n",
        "\n",
        "# With `python_job_elements` defined you can adapt the code in the 'for loop' to iterate over the parent elements instead\n",
        "for job_element in python_job_elements:\n",
        "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
        "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
        "    location_element = job_element.find(\"p\", class_=\"location\")\n",
        "    print(title_element.text.strip())\n",
        "    print(company_element.text.strip())\n",
        "    print(location_element.text.strip())\n",
        "    print()\n",
        "\n",
        "# The `.text` attribute finds only the visible content of an HTML element.\n",
        "# For example, if you are looking for the URL in a link element then start by fetching all the <a> elements.\n",
        "# Then, extract the value of the href attributes using square-bracket notation.\n",
        "\n",
        "for job_element in python_job_elements:\n",
        "    links = job_element.find_all(\"a\")\n",
        "    for link in links:\n",
        "        link_url = link[\"href\"]\n",
        "        print(f\"Apply here: {link_url}\\n\")\n",
        "\n",
        "# To fetch the URL of just the second link for each job card\n",
        "for job_element in python_job_elements:\n",
        "    link_url = job_element.find_all(\"a\")[1][\"href\"]\n",
        "    print(f\"Apply here: {link_url}\\n\")\n",
        "```"
      ],
      "metadata": {
        "id": "XEikCPLbdj6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The web scrapping script"
      ],
      "metadata": {
        "id": "Me_FZlcOdJC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP_csto-4GUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d72e48-7a06-46f5-a48d-d895077a8741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to 'arcticfrenz.json'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import logging\n",
        "from typing import Union\n",
        "\n",
        "def scrape_site(url: str) -> Union[str, None]:\n",
        "  \"\"\"Scrapes the given website and returns the HTML content, or None if the scrape fails.\"\"\"\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    logging.error(\"Failed to scrape website: %s\", url)\n",
        "    return None\n",
        "\n",
        "  if response.status_code == 200:\n",
        "    return response.text\n",
        "  else:\n",
        "    logging.error(\"Received non-200 status code: %s\", response.status_code)\n",
        "    return None\n",
        "\n",
        "def parse_html(html: Union[BeautifulSoup, str]) -> dict:\n",
        "  \"\"\"Parses the given HTML content and returns a dictionary of parsed data, or None if the parsing fails.\"\"\"\n",
        "  if isinstance(html, BeautifulSoup):\n",
        "    soup = html\n",
        "  else:\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "  results = {}\n",
        "\n",
        "  # Extract the data you want from the HTML here\n",
        "  results_container = soup.find(id=\"ResultsContainer\")\n",
        "  for result in results_container.find_all(\"div\", class_=\"card-content\"):\n",
        "    return results\n",
        "\n",
        "def save_data(data: dict, filename: str) -> None:\n",
        "  \"\"\"Saves the given dictionary of data to the given JSON file.\"\"\"\n",
        "  with open(filename, \"w\") as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "def main():\n",
        "  \"\"\"Scrapes the website, parses the HTML, and saves the data.\"\"\"\n",
        "\n",
        "  url = \"https://realpython.github.io/fake-jobs/\"\n",
        "  html = scrape_site(url)\n",
        "\n",
        "  if html:\n",
        "    parsed_data = parse_html(html)\n",
        "    save_data(parsed_data, \"/content/drive/MyDrive/AF/arcticfrenz.json\")\n",
        "    print(\"Data saved to 'arcticfrenz.json'.\")\n",
        "  else:\n",
        "    logging.error(\"Failed to scrape website: %s\", url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ]
}