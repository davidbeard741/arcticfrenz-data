{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oeU0bNjSdB5Q"
      ],
      "mount_file_id": "1JsZICY6bIZ7-T133e_mw12Zv_ehrBKJy",
      "authorship_tag": "ABX9TyNNRS0Qag4gNS94p8y1x++G"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraper"
      ],
      "metadata": {
        "id": "nEcC25TPcacs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Started"
      ],
      "metadata": {
        "id": "2eNjymAuMNaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xNImgY6AIfRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraper With Python Tutorial"
      ],
      "metadata": {
        "id": "oeU0bNjSdB5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web Scraper with Python Tutorial\n",
        "\n",
        "```\n",
        "# Import the Requests library for making HTTP requests\n",
        "import requests\n",
        "\n",
        "# Install Beautiful Soup, a Python library for parsing structured data\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# set the URL to parse\n",
        "URL = \"https://realpython.github.io/fake-jobs/\"\n",
        "\n",
        "# Printing the `.text` attribute of a page provides a holistic, quick look at the HTML.\n",
        "page = requests.get(URL)\n",
        "print(page.text)\n",
        "\n",
        "# To save the page, use the binary `.content` attribute.\n",
        "with open(\"/content/drive/MyDrive/AF/arcticfrenz.html\", \"wb\") as f:\n",
        "    f.write(page.content)\n",
        "\n",
        "# Create a Beautiful Soup object that takes `page.content` as its input.\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "# Find specific HTML elements by its ID\n",
        "results = soup.find(id=\"ResultsContainer\")\n",
        "\n",
        "# For easier viewing, any Beautiful Soup object can use prettify\n",
        "print(results.prettify())\n",
        "\n",
        "\n",
        "# Find elements by HTML class name\n",
        "job_elements = results.find_all(\"div\", class_=\"card-content\")\n",
        "\n",
        "# Pick out child HTML elements with `.find()` attribute.\n",
        "# Each job_element is another BeautifulSoup() object.\n",
        "# `.strip()` can be used to remove the superfluous whitespace\n",
        "for job_element in job_elements:\n",
        "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
        "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
        "    location_element = job_element.find(\"p\", class_=\"location\")\n",
        "    print(title_element.text.strip())\n",
        "    print(company_element.text.strip())\n",
        "    print(location_element.text.strip())\n",
        "    print()\n",
        "\n",
        "# To find elements by class name and text context you can use the string argument\n",
        "# And you can sometimes pass functions as arguments to Beautiful Soup methods\n",
        "python_jobs = results.find_all(\n",
        "    \"h2\", string=lambda text: \"python\" in text.lower()\n",
        ")\n",
        "\n",
        "# A method for checking is to use `len()`\n",
        "print(len(python_jobs))\n",
        "\n",
        "# You can fetch great-grandparent elements\n",
        "python_job_elements = [\n",
        "    h2_element.parent.parent.parent for h2_element in python_jobs\n",
        "]\n",
        "\n",
        "# With `python_job_elements` defined you can adapt the code in the 'for loop' to iterate over the parent elements instead\n",
        "for job_element in python_job_elements:\n",
        "    title_element = job_element.find(\"h2\", class_=\"title\")\n",
        "    company_element = job_element.find(\"h3\", class_=\"company\")\n",
        "    location_element = job_element.find(\"p\", class_=\"location\")\n",
        "    print(title_element.text.strip())\n",
        "    print(company_element.text.strip())\n",
        "    print(location_element.text.strip())\n",
        "    print()\n",
        "\n",
        "# The `.text` attribute finds only the visible content of an HTML element.\n",
        "# For example, if you are looking for the URL in a link element then start by fetching all the <a> elements.\n",
        "# Then, extract the value of the href attributes using square-bracket notation.\n",
        "\n",
        "for job_element in python_job_elements:\n",
        "    links = job_element.find_all(\"a\")\n",
        "    for link in links:\n",
        "        link_url = link[\"href\"]\n",
        "        print(f\"Apply here: {link_url}\\n\")\n",
        "\n",
        "# To fetch the URL of just the second link for each job card\n",
        "for job_element in python_job_elements:\n",
        "    link_url = job_element.find_all(\"a\")[1][\"href\"]\n",
        "    print(f\"Apply here: {link_url}\\n\")\n",
        "```"
      ],
      "metadata": {
        "id": "XEikCPLbdj6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The web scrapping script"
      ],
      "metadata": {
        "id": "Me_FZlcOdJC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import logging\n",
        "from typing import Union, List, Dict\n",
        "\n",
        "def scrape_site(url: str) -> Union[str, None]:\n",
        "    \"\"\"Scrapes the given website and returns the HTML content, or None if the scrape fails.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises a HTTPError for bad responses (4xx and 5xx)\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        logging.error(f\"Failed to scrape website: {url}, due to {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def parse_html(html: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"Parses the given HTML content and returns a list of dictionaries of parsed data, or an empty list if the parsing fails.\"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    results_container = soup.find(id=\"ResultsContainer\")\n",
        "    parsed_data = []\n",
        "\n",
        "    if results_container:\n",
        "        for result in results_container.find_all(\"div\", class_=\"card-content\"):\n",
        "            title = result.find(\"h2\", class_=\"title\").text.strip() if result.find(\"h2\", class_=\"title\") else None\n",
        "            company = result.find(\"h3\", class_=\"company\").text.strip() if result.find(\"h3\", class_=\"company\") else None\n",
        "            location = result.find(\"p\", class_=\"location\").text.strip() if result.find(\"p\", class_=\"location\") else None\n",
        "            parsed_data.append({\"title\": title, \"company\": company, \"location\": location})\n",
        "\n",
        "    return parsed_data\n",
        "\n",
        "def save_data(data: List[Dict[str, str]], filename: str) -> None:\n",
        "    \"\"\"Saves the given list of dictionaries of data to the given JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(filename, \"w\") as file:\n",
        "            json.dump(data, file, indent=4)\n",
        "        print(f\"Data saved to '{filename}'.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to save data to {filename}, due to {str(e)}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Scrapes the website, parses the HTML, and saves the data.\"\"\"\n",
        "    url = \"https://realpython.github.io/fake-jobs/\"\n",
        "    html = scrape_site(url)\n",
        "\n",
        "    if html:\n",
        "        parsed_data = parse_html(html)\n",
        "        if parsed_data:\n",
        "            save_data(parsed_data, \"/content/drive/MyDrive/AF/arcticfrenz.json\")\n",
        "        else:\n",
        "            logging.error(f\"Failed to parse HTML from {url}\")\n",
        "    else:\n",
        "        logging.error(f\"Failed to scrape website: {url}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dv60460UoTTz",
        "outputId": "e1373e43-d993-4575-e940-d8da50aaa0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to '/content/drive/MyDrive/AF/arcticfrenz.json'.\n"
          ]
        }
      ]
    }
  ]
}